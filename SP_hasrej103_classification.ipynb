{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZGxZk68JFetq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import csr_matrix, issparse\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_imbalanced_data(df, target_col=\"hasrej103_x\"):\n",
        "    \"\"\"\n",
        "    1. Creates a binary rejection column ('reject_hasrej103').\n",
        "    2. Splits data into 70% Train, 15% Validation, 15% Test (stratified).\n",
        "    3. Downsamples the majority class in the training set to achieve a 50/50 balance.\n",
        "    4. Ensures the validation and test sets mimic the original class distribution.\n",
        "\n",
        "    :param df: The input DataFrame containing the target column.\n",
        "    :param target_col: The column to use for the target label (e.g., \"hasrej103\").\n",
        "    :return: Indices and labels for the balanced training set, and the real-world val/test sets.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Select relevant column and create the binary target\n",
        "    #df_claims = df[[target_col]].copy()\n",
        "\n",
        "    # Create the new binary column: reject_hasrej103 (1 if >= 1, else 0)\n",
        "    df_claims[\"reject_hasrej103\"] = (df_claims[target_col].fillna(0) >= 1).astype(int)\n",
        "\n",
        "    # Separate index (X) and the new target label (y)\n",
        "    X = df_claims.index.to_series() # Use index as 'X' for splitting\n",
        "    y = df_claims[\"reject_hasrej103\"]\n",
        "\n",
        "    print(\"--- Initial Data Status ---\")\n",
        "    print(f\"Total Samples: {len(df_claims)}\")\n",
        "    print(f\"Class Distribution:\")\n",
        "\n",
        "    # 2. Split into Train (70%) and Temp (30%), stratified\n",
        "    X_train_idx, X_temp_idx, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Split Temp (30%) into Validation (15% of total) and Test (15% of total), stratified\n",
        "    X_val_idx, X_test_idx, y_val, y_test = train_test_split(\n",
        "        X_temp_idx, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        "    )\n",
        "\n",
        "    # Function to print information\n",
        "    def print_split_info(name, y_data):\n",
        "        count = len(y_data)\n",
        "        # Recalculate distribution directly from the balanced/split labels\n",
        "        dist = y_data.value_counts(normalize=True).mul(100).round(2)\n",
        "        # Use .get() for safety\n",
        "        class_1_perc = dist.get(1, 0.0)\n",
        "        class_0_perc = dist.get(0, 0.0)\n",
        "        print(f\"**{name}** (N={count}): {class_1_perc}% Class 1 | {class_0_perc}% Class 0\")\n",
        "\n",
        "    #print_split_info(\"Train (Balanced)\", y_train_balanced, train_indices_balanced)\n",
        "    # The val/test sets will still show the real-world distribution\n",
        "    print_split_info(\"Train\", y_train)\n",
        "    print_split_info(\"Validation (Real-World)\", y_val)\n",
        "    print_split_info(\"Test (Real-World)\", y_test)\n",
        "\n",
        "   return X_train_idx, X_val_idx.index, X_test_idx.index, y_train, y_val, y_test"
      ],
      "metadata": {
        "id": "mg9rbBA6Ss3L"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Define File Paths ---\n",
        "# **UPDATE THESE PATHS TO YOUR ACTUAL FILE LOCATIONS**\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "LABEL_CSV = \"all_features_df.csv\"\n",
        "TARGET_COLUMN = \"hasrej103_x\"\n",
        "FEATURES_JOBLIB = \"/content/drive/MyDrive/MSAI 339 - Data Science/tfidf_features.joblib\"\n",
        "SAVE_MODEL_PATH = \"xgboost_103_model.joblib\"\n",
        "\n",
        "# --- 2. Load Data ---\n",
        "print(\"--- Loading Raw Data for Splitting ---\")\n",
        "\"\"\"try:\n",
        "  # Load the raw data to get the index and target column\n",
        "  raw_df = pd.read_csv(LABEL_CSV, usecols=[TARGET_COLUMN])\n",
        "except Exception as e:\n",
        "  print(f\"Error loading raw data from {LABEL_CSV}: {e}\")\n",
        "  # --- Fallback to Simulated Data (For testing if files aren't available) ---\n",
        "  print(\"Using a simulated DataFrame for demonstration...\")\n",
        "  N_total = 1000\n",
        "  raw_df = pd.DataFrame({\n",
        "      TARGET_COLUMN: ([0] * 800) + ([1] * 200) # 80/20 split\n",
        "  }).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "  raw_df.index.name = 'original_index'\n",
        "\"\"\"\n",
        "#df = pd.read_csv('/content/drive/MyDrive/MSAI 339 - Data Science/pg_claims_tokenized.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/MSAI 339 - Data Science/combined_df.csv')\n",
        "df = df.reset_index(drop=True)\n",
        "df_claims = df[[\"claim_text\", \"hasrej101\", \"hasrej102\", \"hasrej103\", \"hasrej112\"]]\n",
        "# Perform the custom data split and downsampling\n",
        "train_indices_b, val_indices, test_indices, y_train_b, y_val, y_test = prepare_imbalanced_data(\n",
        "  None,\n",
        "  target_col=TARGET_COLUMN\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "izZjN36XStzp",
        "outputId": "7ad35220-0cd3-4a0d-a176-5796e5bd148c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "--- Loading Raw Data for Splitting ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1838491837.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \"\"\"\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#df = pd.read_csv('/content/drive/MyDrive/MSAI 339 - Data Science/pg_claims_tokenized.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/MSAI 339 - Data Science/combined_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdf_claims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"claim_text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hasrej101\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hasrej102\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hasrej103\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hasrej112\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_COLUMN = \"hasrej103_x\"\n",
        "df_claims = df[[\"claim_text\", \"hasrej101_x\", \"hasrej102_x\", \"hasrej103_x\", \"hasrej112_x\"]]\n",
        "# Perform the custom data split and downsampling\n",
        "print(df_claims.head(20))\n",
        "train_indices_b, val_indices, test_indices, y_train_b, y_val, y_test = prepare_imbalanced_data(\n",
        "  None,\n",
        "  target_col=TARGET_COLUMN\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeQjG4dn9ZQU",
        "outputId": "39699b10-e577-458f-edf9-fa081af9c504"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           claim_text  hasrej101_x  \\\n",
            "0   ['nonamplifi', 'detect', 'polynucleotid', 'pro...          1.0   \n",
            "1   ['compound', 'follow', 'formula', 'pharmaceut'...          0.0   \n",
            "2   ['method', 'target', 'genom', 'modif', 'within...          0.0   \n",
            "3   ['mammalian', 'cell', 'line', 'compris', 'firs...          0.0   \n",
            "4   ['method', 'determin', 'amount', 'andor', 'con...          0.0   \n",
            "5   ['isol', 'antibodi', 'antigen', 'bind', 'porti...          0.0   \n",
            "6   ['insulin', 'receptor', 'aptam', 'compris', 'n...          0.0   \n",
            "7   ['method', 'oper', 'electron', 'regul', 'retur...          0.0   \n",
            "8   ['vaccin', 'composit', 'remov', 'boar', 'taint...          0.0   \n",
            "9   ['cell', 'lack', 'function', 'express', 'mhc',...          0.0   \n",
            "10  ['compstatin', 'analogu', 'repres', 'formula',...          NaN   \n",
            "11  ['apparatu', 'compris', 'transceiv', 'commun',...          0.0   \n",
            "12  ['heavi', 'chain', 'variabl', 'region', 'antib...          0.0   \n",
            "13  ['imag', 'form', 'apparatu', 'compris', 'feede...          0.0   \n",
            "14  ['recombin', 'oncolyt', 'viru', 'compris', 'nu...          0.0   \n",
            "15  ['method', 'select', 'kill', 'one', 'cancer', ...          0.0   \n",
            "16  ['multispecif', 'bind', 'protein', 'compris', ...          1.0   \n",
            "17  ['light', 'devic', 'vehicl', 'compris', 'plura...          0.0   \n",
            "18  ['recombin', 'factor', 'viii', 'protein', 'com...          0.0   \n",
            "19  ['move', 'robot', 'compris', 'bodi', 'form', '...          0.0   \n",
            "\n",
            "    hasrej102_x  hasrej103_x  hasrej112_x  \n",
            "0           1.0          1.0          1.0  \n",
            "1           1.0          1.0          1.0  \n",
            "2           1.0          1.0          1.0  \n",
            "3           1.0          1.0          1.0  \n",
            "4           1.0          1.0          1.0  \n",
            "5           1.0          0.0          1.0  \n",
            "6           0.0          0.0          1.0  \n",
            "7           1.0          1.0          1.0  \n",
            "8           0.0          1.0          1.0  \n",
            "9           1.0          1.0          1.0  \n",
            "10          NaN          NaN          NaN  \n",
            "11          1.0          0.0          0.0  \n",
            "12          1.0          0.0          1.0  \n",
            "13          0.0          1.0          1.0  \n",
            "14          0.0          0.0          1.0  \n",
            "15          1.0          1.0          0.0  \n",
            "16          0.0          0.0          1.0  \n",
            "17          1.0          1.0          1.0  \n",
            "18          1.0          0.0          1.0  \n",
            "19          1.0          1.0          1.0  \n",
            "--- Initial Data Status ---\n",
            "Total Samples: 3909923\n",
            "Initial Class Distribution:\n",
            "reject_hasrej103\n",
            "1    67.98%\n",
            "0    32.02%\n",
            "Name: proportion, dtype: object\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1961899189.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_claims[\"reject_hasrej103\"] = (df_claims[target_col].fillna(0) >= 1).astype(int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Downsampling Training Set ---\n",
            "Majority Class Label: 1 (Downsampled)\n",
            "Minority Class Label: 0 (Retained)\n",
            "Downsampled Count (50%): 876259\n",
            "------------------------------\n",
            "--- Final Set Sizes & Distributions ---\n",
            "**Validation (Real-World)** (N=586488): 67.98% Class 1 | 32.02% Class 0\n",
            "**Test (Real-World)** (N=586489): 67.98% Class 1 | 32.02% Class 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeHPpEh17MCL",
        "outputId": "d97e1714-aaed-45cd-ca87-dea170fc03a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['application_number', 'entity_size_MICRO', 'entity_size_SMALL',\n",
              "       'entity_size_UNDISCOUNTED', 'aia_first_to_file_False',\n",
              "       'aia_first_to_file_True', 'has_rej101', 'has_rej102', 'has_rej103',\n",
              "       'has_rej112', 'final_outcome_x', 'Rejected', 'pgpub_id_x',\n",
              "       'patent_id_x', 'claim_text', 'hasrej101_x', 'hasrej102_x',\n",
              "       'hasrej103_x', 'hasrej112_x', 'submissionDate_x',\n",
              "       'groupartunitnumber_x', 'Unnamed: 0', 'application_invention_type',\n",
              "       'examiner_full_name', 'examiner_art_unit', 'uspc_class',\n",
              "       'uspc_subclass', 'confirm_number', 'atty_docket_number',\n",
              "       'appl_status_desc', 'appl_status_date', 'file_location',\n",
              "       'earliest_pgpub_number', 'earliest_pgpub_date', 'patent_number',\n",
              "       'patent_issue_date', 'invention_title', 'small_entity_indicator',\n",
              "       'aia_first_to_file', 'applicant_organization', 'geographical_region',\n",
              "       'pgpub_id_y', 'application_id', 'patent_id_y',\n",
              "       'patentApplicationNumber', 'hasrej101_y', 'hasrej102_y', 'hasrej103_y',\n",
              "       'hasrej112_y', 'submissionDate_y', 'groupartunitnumber_y',\n",
              "       'step_sequence', 'number_of_oa', 'unique_rej_types', 'final_outcome_y',\n",
              "       'num_office_actions', 'oa_span_days', 'mean_gap_days', 'min_gap_days',\n",
              "       'max_gap_days', 'num_oa_with_101', 'num_oa_with_102', 'num_oa_with_103',\n",
              "       'num_oa_with_112', 'max_rej_types_in_oa', 'avg_rej_types_per_oa',\n",
              "       'first_has_101', 'first_has_102', 'first_has_103', 'first_has_112',\n",
              "       'last_has_101', 'last_has_102', 'last_has_103', 'last_has_112',\n",
              "       'y_approved', 'y_101', 'y_102', 'y_103', 'y_112', 'total_rejections',\n",
              "       'app_status_year', 'app_status_month', 'pub_year', 'years_since_pub',\n",
              "       'examiner_base', 'examiner_final', 'geo_raw', 'geo_city', 'geo_region',\n",
              "       'geo_country', 'country_approval_rate', 'region_approval_rate',\n",
              "       'geo_city_freq', 'examiner_approval_rate', 'artunit_approval_rate',\n",
              "       'uspc_class_freq', 'uspc_subclass_freq', 'uspc_class_success',\n",
              "       'uspc_subclass_success'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the sparse features\n",
        "print(\"\\n--- Loading Sparse TF-IDF Features ---\")\n",
        "try:\n",
        "    X_all = joblib.load(FEATURES_JOBLIB)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading features from {FEATURES_JOBLIB}: {e}\")\n",
        "    # Exit if features can't be loaded, as the classifier requires them\n",
        "    exit()\n",
        "\n",
        "# Ensure X is a sparse matrix\n",
        "if not issparse(X_all):\n",
        "    X_all = csr_matrix(X_all)\n",
        "    print(\"Features converted to sparse matrix.\")\n",
        "\n",
        "print(f\"Sparse matrix shape loaded: {X_all.shape}\")\n",
        "\n",
        "# Map the indices to the sparse feature matrix\n",
        "X_train = X_all[train_indices_b, :]\n",
        "X_test = X_all[test_indices, :]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HmCREzv-ktC",
        "outputId": "1cca2992-74e7-42a0-814d-a36d87cad0c0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading Sparse TF-IDF Features ---\n",
            "Sparse matrix shape loaded: (3909923, 637906)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r2klroi-8NA",
        "outputId": "814351e4-3788-4d9c-f07f-ee3243456f0c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1752518, 637906)\n",
            "(586489, 637906)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the sparse features\n",
        "print(\"\\n--- Loading Sparse TF-IDF Features ---\")\n",
        "try:\n",
        "    X_all = joblib.load(FEATURES_JOBLIB)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading features from {FEATURES_JOBLIB}: {e}\")\n",
        "    # Exit if features can't be loaded, as the classifier requires them\n",
        "    exit()\n",
        "\n",
        "# Ensure X is a sparse matrix\n",
        "if not issparse(X_all):\n",
        "    X_all = csr_matrix(X_all)\n",
        "    print(\"Features converted to sparse matrix.\")\n",
        "\n",
        "print(f\"Sparse matrix shape loaded: {X_all.shape}\")\n",
        "\n",
        "# Map the indices to the sparse feature matrix\n",
        "X_train = X_all[train_indices_b, :]\n",
        "X_test = X_all[test_indices, :]\n",
        "X_val = X_all[val_indices, :]\n",
        "\n",
        "# --- 3. Train XGBoost Model ---\n",
        "print(\"\\n--- Training XGBoost Classifier ---\")\n",
        "\n",
        "# Calculate the scale_pos_weight for handling imbalanced classes\n",
        "num_pos = (y_train_b == 1).sum()\n",
        "num_neg = (y_train_b == 0).sum()\n",
        "# Since we explicitly balanced the training set to 50/50, scale_pos_weight will be 1.0,\n",
        "# but we calculate it here for robustness if the split ratio changes.\n",
        "scale_pos_weight_value = num_neg / num_pos\n",
        "print(f\"Positive class weight (scale_pos_weight) set to: {scale_pos_weight_value:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN3uEOrPWFlE",
        "outputId": "a44bb447-fcd1-4aad-be2c-2f91fa50ddb7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading Sparse TF-IDF Features ---\n",
            "Sparse matrix shape loaded: (3909923, 637906)\n",
            "\n",
            "--- Training XGBoost Classifier ---\n",
            "Positive class weight (scale_pos_weight) set to: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **XGBoost Classifier Definition**\n",
        "model = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    n_estimators=100,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.1,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    #scale_pos_weight=scale_pos_weight_value\n",
        ")\n",
        "\n",
        "# Train the model using the balanced feature set\n",
        "model.fit(X_train, y_train_b)\n",
        "\n",
        "# --- 4. Save Model ---\n",
        "joblib.dump(model, SAVE_MODEL_PATH)\n",
        "print(f\"\\nModel saved to: {SAVE_MODEL_PATH}\")\n",
        "\n",
        "# --- 5. Evaluate Model ---\n",
        "print(\"\\n--- Evaluating XGBoost Model on Test Set (Real-World Distribution) ---\")\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Print the full classification report\n",
        "print(classification_report(y_test, predictions))\n"
      ],
      "metadata": {
        "id": "iLq9sl82WvxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a66e78-7ef0-4f96-fd73-5220c9115792"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:47] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model saved to: xgboost_103_model.joblib\n",
            "\n",
            "--- Evaluating XGBoost Model on Test Set (Real-World Distribution) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.15      0.24    187770\n",
            "           1       0.71      0.96      0.81    398719\n",
            "\n",
            "    accuracy                           0.70    586489\n",
            "   macro avg       0.67      0.55      0.53    586489\n",
            "weighted avg       0.68      0.70      0.63    586489\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_val = model.predict(X_val)\n",
        "print(classification_report(y_val, predictions_val))\n",
        "print('---------')\n",
        "predictions_train = model.predict(X_train)\n",
        "print(classification_report(y_train_b, predictions_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4AXETdCHayG",
        "outputId": "15a1dad4-9387-4133-8245-412d27a8d59a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.15      0.24    187769\n",
            "           1       0.71      0.96      0.81    398719\n",
            "\n",
            "    accuracy                           0.70    586488\n",
            "   macro avg       0.67      0.56      0.53    586488\n",
            "weighted avg       0.68      0.70      0.63    586488\n",
            "\n",
            "---------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.16      0.25    876259\n",
            "           1       0.71      0.96      0.82   1860687\n",
            "\n",
            "    accuracy                           0.70   2736946\n",
            "   macro avg       0.68      0.56      0.53   2736946\n",
            "weighted avg       0.69      0.70      0.64   2736946\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  # --- Optional: Naive Bayes implementation ---\n",
        "  # To run Naive Bayes, uncomment the imports at the top and the code below.\n",
        "  # print(\"\\n--- Optional: Multinomial Naive Bayes Classifier ---\")\n",
        "  nb_model = MultinomialNB()\n",
        "  nb_model.fit(X_train, y_train_b) # Train on the same balanced data\n",
        "  nb_predictions = nb_model.predict(X_test)\n",
        "  print(classification_report(y_test, nb_predictions))\n",
        "  print('------------')\n",
        "  nb_val_predictions = nb_model.predict(X_val)\n",
        "  print(classification_report(y_val, nb_val_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLBHhYCHWIYD",
        "outputId": "da86fceb-e71c-4ee3-e5b4-44670421f4f9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.08      0.15    187770\n",
            "           1       0.69      0.97      0.81    398719\n",
            "\n",
            "    accuracy                           0.69    586489\n",
            "   macro avg       0.65      0.53      0.48    586489\n",
            "weighted avg       0.66      0.69      0.60    586489\n",
            "\n",
            "------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.08      0.15    187769\n",
            "           1       0.69      0.97      0.81    398719\n",
            "\n",
            "    accuracy                           0.69    586488\n",
            "   macro avg       0.65      0.53      0.48    586488\n",
            "weighted avg       0.66      0.69      0.60    586488\n",
            "\n"
          ]
        }
      ]
    }
  ]
}